{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to my WineDoc!","title":"Home"},{"location":"#welcome-to-my-winedoc","text":"","title":"Welcome to my WineDoc!"},{"location":"mkdocs/mkdocs-getting-started-with-docker/","text":"Getting started with mkdocs and material In order to set up a development environment, follow the steps below. Step 1. Download de docker image >> docker pull squidfunk/mkdocs-material Step 2. Create a folder to host your project Step 3. Start an empty project docker run --rm -it -v ${PWD}:/docs squidfunk/mkdocs-material new . Step 4. Create a docker-compose.yml file to declare the development server container version: '3' services: documentation.dev: image: squidfunk/mkdocs-material ports: - '8000:8000' volumes: - ./mydoc:/docs Step 5. Deploy the development serve r >> docker-compose up Step 6. Access the development server in the url http://localhost:8000 Step 7. Enjoy creating your Doc! Step 8. Once you have finished your Doc, build the static files. >> mkdocs build","title":"Getting Started"},{"location":"mkdocs/mkdocs-getting-started-with-docker/#getting-started-with-mkdocs-and-material","text":"In order to set up a development environment, follow the steps below.","title":"Getting started with mkdocs and material"},{"location":"mkdocs/mkdocs-getting-started-with-docker/#step-1-download-de-docker-image","text":">> docker pull squidfunk/mkdocs-material","title":"Step 1. Download de docker image"},{"location":"mkdocs/mkdocs-getting-started-with-docker/#step-2-create-a-folder-to-host-your-project","text":"","title":"Step 2. Create a folder to host your project"},{"location":"mkdocs/mkdocs-getting-started-with-docker/#step-3-start-an-empty-project","text":"docker run --rm -it -v ${PWD}:/docs squidfunk/mkdocs-material new .","title":"Step 3. Start an empty project"},{"location":"mkdocs/mkdocs-getting-started-with-docker/#step-4-create-a-docker-composeyml-file-to-declare-the-development-server-container","text":"version: '3' services: documentation.dev: image: squidfunk/mkdocs-material ports: - '8000:8000' volumes: - ./mydoc:/docs","title":"Step 4. Create a docker-compose.yml file to declare the development server container"},{"location":"mkdocs/mkdocs-getting-started-with-docker/#step-5-deploy-the-development-server","text":">> docker-compose up","title":"Step 5. Deploy the development server"},{"location":"mkdocs/mkdocs-getting-started-with-docker/#step-6-access-the-development-server-in-the-url-httplocalhost8000","text":"","title":"Step 6. Access the development server in the url http://localhost:8000"},{"location":"mkdocs/mkdocs-getting-started-with-docker/#step-7-enjoy-creating-your-doc","text":"","title":"Step 7. Enjoy creating your Doc!"},{"location":"mkdocs/mkdocs-getting-started-with-docker/#step-8-once-you-have-finished-your-doc-build-the-static-files","text":">> mkdocs build","title":"Step 8. Once you have finished your Doc, build the static files."},{"location":"udemy/docker/cap01-dive-into-docker/","text":"Section 1 - Dive into docker Aulas 3-4 Docker \u00e9 uma plataforma que facilita a implanta\u00e7\u00e3o de m\u00faltiplos softwares em uma mesma m\u00e1quina, isolados entre si, em uma estrutura conhecida como Container. O docker \u00e9 formado por dois componentes: Docker CLI \u00c9 a linha de comando que permite a comunica\u00e7\u00e3o com o Docker Daemon Docker Daemon \u00c9 a engine do Docker, respons\u00e1vel por criar imagens, implantar containeres etc... Aula 12 Container \u00e9 um ambiente isolado, dentro do qual um processo \u00e9 executado. A implementa\u00e7\u00e3o do container \u00e9 feita por dois recursos: namespaces cgroups namespace \u00e9 um artif\u00edcio, DO LINUX, usado para fazer a segmenta\u00e7\u00e3o de recursos, tais como: espa\u00e7o em disco, comunica\u00e7\u00e3o em rede, processos, usu\u00e1ros e hostnames. Isso significa que: um processo rodando dentro de uma namespace s\u00f3 consegue enxergar e se comunicar com outros processos que estejam dentro dessa mesma namespace. todas as chamadas para escrita em disco, que esse processo fizer, ser\u00e3o redirecionadas (pelo kernel) para o espa\u00e7o que foi mapeado para a namespace onde o processo est\u00e1 contido. o processo s\u00f3 enxerga as interfaces de rede que est\u00e3o na namespace o processo s\u00f3 enxerga os usu\u00e1rios que est\u00e3o definidos na namespace cgroup \u00e9 outro artif\u00edcio, DO LINUX, de segmenta\u00e7\u00e3o, mas que permite a isola\u00e7\u00e3o de recursos de hardware, tais como: mem\u00f3ria RAM, uso de CPU, taxa de leitura escrita em disco, largura de banda etc... A combina\u00e7\u00e3o de namespaces e cgroups permite criar, em uma mesma m\u00e1quina, ambientes onde diferentes processos (softwares) operam de forma isolada. PS: A m\u00e1quina hospedeira continua enxergando tudo como processos rodando sobre o sistema operacional. O Kernel que faz a m\u00e1gica de isolar os processos! (foi o que entendi). Aula 13 Uma imagem de um container \u00e9 uma Snapshot de um sistema de arquivos. Dentro desse sistema de arquivos est\u00e3o todos os arquivos necess\u00e1rios para a execu\u00e7\u00e3o de um sofware em espec\u00edfico, seguido de um comando que deve ser executado para que esse software entre em execu\u00e7\u00e3o como um processo. Quando um container \u00e9 implantado a partir de uma imagem, a snapshot do sistema de arquivos \u00e9 montada dentro do disco, especificamente no espa\u00e7o que foi mapeado para a namespace onde a aplica\u00e7\u00e3o vai rodar.Ap\u00f3s isso, o comando de execu\u00e7\u00e3o da aplica\u00e7\u00e3o \u00e9 executado.","title":"**Section 1 - Dive into docker**"},{"location":"udemy/docker/cap01-dive-into-docker/#section-1-dive-into-docker","text":"","title":"Section 1 - Dive into docker"},{"location":"udemy/docker/cap01-dive-into-docker/#aulas-3-4","text":"Docker \u00e9 uma plataforma que facilita a implanta\u00e7\u00e3o de m\u00faltiplos softwares em uma mesma m\u00e1quina, isolados entre si, em uma estrutura conhecida como Container. O docker \u00e9 formado por dois componentes: Docker CLI \u00c9 a linha de comando que permite a comunica\u00e7\u00e3o com o Docker Daemon Docker Daemon \u00c9 a engine do Docker, respons\u00e1vel por criar imagens, implantar containeres etc...","title":"Aulas 3-4"},{"location":"udemy/docker/cap01-dive-into-docker/#aula-12","text":"Container \u00e9 um ambiente isolado, dentro do qual um processo \u00e9 executado. A implementa\u00e7\u00e3o do container \u00e9 feita por dois recursos: namespaces cgroups namespace \u00e9 um artif\u00edcio, DO LINUX, usado para fazer a segmenta\u00e7\u00e3o de recursos, tais como: espa\u00e7o em disco, comunica\u00e7\u00e3o em rede, processos, usu\u00e1ros e hostnames. Isso significa que: um processo rodando dentro de uma namespace s\u00f3 consegue enxergar e se comunicar com outros processos que estejam dentro dessa mesma namespace. todas as chamadas para escrita em disco, que esse processo fizer, ser\u00e3o redirecionadas (pelo kernel) para o espa\u00e7o que foi mapeado para a namespace onde o processo est\u00e1 contido. o processo s\u00f3 enxerga as interfaces de rede que est\u00e3o na namespace o processo s\u00f3 enxerga os usu\u00e1rios que est\u00e3o definidos na namespace cgroup \u00e9 outro artif\u00edcio, DO LINUX, de segmenta\u00e7\u00e3o, mas que permite a isola\u00e7\u00e3o de recursos de hardware, tais como: mem\u00f3ria RAM, uso de CPU, taxa de leitura escrita em disco, largura de banda etc... A combina\u00e7\u00e3o de namespaces e cgroups permite criar, em uma mesma m\u00e1quina, ambientes onde diferentes processos (softwares) operam de forma isolada. PS: A m\u00e1quina hospedeira continua enxergando tudo como processos rodando sobre o sistema operacional. O Kernel que faz a m\u00e1gica de isolar os processos! (foi o que entendi).","title":"Aula 12"},{"location":"udemy/docker/cap01-dive-into-docker/#aula-13","text":"Uma imagem de um container \u00e9 uma Snapshot de um sistema de arquivos. Dentro desse sistema de arquivos est\u00e3o todos os arquivos necess\u00e1rios para a execu\u00e7\u00e3o de um sofware em espec\u00edfico, seguido de um comando que deve ser executado para que esse software entre em execu\u00e7\u00e3o como um processo. Quando um container \u00e9 implantado a partir de uma imagem, a snapshot do sistema de arquivos \u00e9 montada dentro do disco, especificamente no espa\u00e7o que foi mapeado para a namespace onde a aplica\u00e7\u00e3o vai rodar.Ap\u00f3s isso, o comando de execu\u00e7\u00e3o da aplica\u00e7\u00e3o \u00e9 executado.","title":"Aula 13"},{"location":"udemy/docker/cap02-manipulating-containers/","text":"Section 2 - Manipulating Containers Aula 21 No sistema operacional do Linux, o Kernel pode enviar dois tipos de sinais para parar um processo (uma aplica\u00e7\u00e3o rodando). S\u00e3o eles: SIGTERM Esse sinal informa ao processo que ele deve ser encerrado dentro de um intervalo de tempo. Isso permite ao processo realizar alguma opera\u00e7\u00e3o de backup de dados, persist\u00eancia de arquivos, ou mesmo de limpeza (clean-up) antes de ser encerrado. SIGKILL Esse sinal \u00e9 enviado para fechar o processo imediatamente. Tudo que ele tiver produzido de dados e que n\u00e3o tiver sido salvo, ser\u00e1 perdido. Os processos que rodam dentro das namespaces tamb\u00e9m s\u00e3o encerrados via esses dois sinais. O comando \"docker stop containerID\" faz o kernel enviar um SIGTERM para o processo principal dentro do container. O comando \"docker kill containerID\" faz o kernel enviar um SIGKILL para o processo principal dentro do container. Aulas 23 - 25 Todo processo do linux possui tr\u00eas interfaces com o sistema operacional: STDIN Essa \u00e9 a interface de entrada. Toda informa\u00e7\u00e3o que chega ao processo entra por essa interface Quando um comando \u00e9 enviado a um processo, ele chega neste pelo STDIN STDOUT Esse \u00e9 o ponto de sa\u00edda de dados e informa\u00e7\u00f5es do processo Todo log, ou outro tipo de informa\u00e7\u00e3o que o processo responde, \u00e9 retornado pelo STDOUT STDERR Esse \u00e9 o canal usado pelo processo para comunicar erros internos. O mesmo vale para um processo rodando dentro de um container. Existem comandos do docker que permitem conectar o terminal da sua m\u00e1quina local \u00e0 interface STDIN do processo que est\u00e1 dentro do container. Dessa forma, se o processo que est\u00e1 dentro do container for um outro terminal (shell), \u00e9 poss\u00edvel rodar comandos dentro do container a partir do terminal local da m\u00e1quina host.","title":"**Section 2 - Manipulating Containers**"},{"location":"udemy/docker/cap02-manipulating-containers/#section-2-manipulating-containers","text":"","title":"Section 2 - Manipulating Containers"},{"location":"udemy/docker/cap02-manipulating-containers/#aula-21","text":"No sistema operacional do Linux, o Kernel pode enviar dois tipos de sinais para parar um processo (uma aplica\u00e7\u00e3o rodando). S\u00e3o eles: SIGTERM Esse sinal informa ao processo que ele deve ser encerrado dentro de um intervalo de tempo. Isso permite ao processo realizar alguma opera\u00e7\u00e3o de backup de dados, persist\u00eancia de arquivos, ou mesmo de limpeza (clean-up) antes de ser encerrado. SIGKILL Esse sinal \u00e9 enviado para fechar o processo imediatamente. Tudo que ele tiver produzido de dados e que n\u00e3o tiver sido salvo, ser\u00e1 perdido. Os processos que rodam dentro das namespaces tamb\u00e9m s\u00e3o encerrados via esses dois sinais. O comando \"docker stop containerID\" faz o kernel enviar um SIGTERM para o processo principal dentro do container. O comando \"docker kill containerID\" faz o kernel enviar um SIGKILL para o processo principal dentro do container.","title":"Aula 21"},{"location":"udemy/docker/cap02-manipulating-containers/#aulas-23-25","text":"Todo processo do linux possui tr\u00eas interfaces com o sistema operacional: STDIN Essa \u00e9 a interface de entrada. Toda informa\u00e7\u00e3o que chega ao processo entra por essa interface Quando um comando \u00e9 enviado a um processo, ele chega neste pelo STDIN STDOUT Esse \u00e9 o ponto de sa\u00edda de dados e informa\u00e7\u00f5es do processo Todo log, ou outro tipo de informa\u00e7\u00e3o que o processo responde, \u00e9 retornado pelo STDOUT STDERR Esse \u00e9 o canal usado pelo processo para comunicar erros internos. O mesmo vale para um processo rodando dentro de um container. Existem comandos do docker que permitem conectar o terminal da sua m\u00e1quina local \u00e0 interface STDIN do processo que est\u00e1 dentro do container. Dessa forma, se o processo que est\u00e1 dentro do container for um outro terminal (shell), \u00e9 poss\u00edvel rodar comandos dentro do container a partir do terminal local da m\u00e1quina host.","title":"Aulas 23 - 25"},{"location":"udemy/docker/cap03-building-custom-images/","text":"Section 3 - Building custom images In order to build your own docker images, you need to create a dockerfile which is a kind of a recipe. A basic implementation of a dockerfile would be as the following: # Imagem base FROM alpine # Executa comando RUN apk add --update redis # Comando de execu\u00e7\u00e3o do processo principal CMD [\"redis-server\"] After you have finished writing the dockerfile, the next step is to build a docker image from it. For that, there is the following command: >> docker build . Now, let's break the dockerfile down, line by line. FROM alpine Nessa linha define-se uma imagem base, a partir da qual a imagem da aplica\u00e7\u00e3o ser\u00e1 constru\u00edda. Essa imagem base j\u00e1 vem com o sistema de arquivos de um sistema linux, com as funcionalidades essenciais. Alpine \u00e9 uma vers\u00e3o do linux extremamente leve. Ela s\u00f3 possu\u00ed os arquivos que s\u00e3o necess\u00e1rios para o funcionamento do SO. Nada al\u00e9m disso . RUN apk add --update redis apk \u00e9 o gerenciador de pacotes do Alpine. Com ele \u00e9 poss\u00edvel adicionar softwares de terceiros \u00e0 imagem que est\u00e1 para ser constru\u00edda. Neste caso, o gerenciador est\u00e1 sendo usado para instalar o redis. CMD [\"redis-server\"] Depois de os softwares requeridos pela aplica\u00e7\u00e3o terem sido instalados (incluindo a pr\u00f3pria aplica\u00e7\u00e3o), \u00e9 preciso explicitar qual ser\u00e1 o comando usado para iniciar a aplica\u00e7\u00e3o como um processo dentro do container. Como a aplica\u00e7\u00e3o \u00e9 o redis (neste caso), o comando \u00e9 \"redis-server\". >> docker build . Comando para construir a imagem a partir do dockerfile. O processo de build consiste de etapas. A primeira etapa quem faz \u00e9 o Docker Server. Ela consiste basicamente em verificar se a imagem base se encontra na m\u00e1quina local (no cache de imagens), caso negativo, o Docker Server vai baixar ela do reposit\u00f3rio. Cada comando RUN consiste de uma nova etapa. No primeiro comando RUN, um container ser\u00e1 criado com a imagem base. Uma vez montado, o comando explicitado ser\u00e1 executado e o sistema de arquivos, que at\u00e9 ent\u00e3o era o da imagem base, ser\u00e1 transformado para um novo estado (seja pela adi\u00e7\u00e3o de novas pastas e arquivos, seja pela instala\u00e7\u00e3o de novos pacotes). Ao final, uma nova imagem ser\u00e1 criada, dessa vez contendo o atual estado do sistema de arquivos. No segundo comando RUN o mesmo processo vai ocorrer. A \u00fanica diferen\u00e7a \u00e9 que o container vai ser criado a partir da imagem que foi gerada na etapa imediatamente anterior! (descrita no par\u00e1grafo acima). Ap\u00f3s todos os comandos RUN, a imagem resultante do processo (retornada ap\u00f3s a execu\u00e7\u00e3o do \u00faltimo comando RUN) vai ser configurada com o comando de execu\u00e7\u00e3o do processo principal.","title":"**Section 3 - Building custom images**"},{"location":"udemy/docker/cap03-building-custom-images/#section-3-building-custom-images","text":"In order to build your own docker images, you need to create a dockerfile which is a kind of a recipe. A basic implementation of a dockerfile would be as the following: # Imagem base FROM alpine # Executa comando RUN apk add --update redis # Comando de execu\u00e7\u00e3o do processo principal CMD [\"redis-server\"] After you have finished writing the dockerfile, the next step is to build a docker image from it. For that, there is the following command: >> docker build . Now, let's break the dockerfile down, line by line. FROM alpine Nessa linha define-se uma imagem base, a partir da qual a imagem da aplica\u00e7\u00e3o ser\u00e1 constru\u00edda. Essa imagem base j\u00e1 vem com o sistema de arquivos de um sistema linux, com as funcionalidades essenciais. Alpine \u00e9 uma vers\u00e3o do linux extremamente leve. Ela s\u00f3 possu\u00ed os arquivos que s\u00e3o necess\u00e1rios para o funcionamento do SO. Nada al\u00e9m disso . RUN apk add --update redis apk \u00e9 o gerenciador de pacotes do Alpine. Com ele \u00e9 poss\u00edvel adicionar softwares de terceiros \u00e0 imagem que est\u00e1 para ser constru\u00edda. Neste caso, o gerenciador est\u00e1 sendo usado para instalar o redis. CMD [\"redis-server\"] Depois de os softwares requeridos pela aplica\u00e7\u00e3o terem sido instalados (incluindo a pr\u00f3pria aplica\u00e7\u00e3o), \u00e9 preciso explicitar qual ser\u00e1 o comando usado para iniciar a aplica\u00e7\u00e3o como um processo dentro do container. Como a aplica\u00e7\u00e3o \u00e9 o redis (neste caso), o comando \u00e9 \"redis-server\". >> docker build . Comando para construir a imagem a partir do dockerfile. O processo de build consiste de etapas. A primeira etapa quem faz \u00e9 o Docker Server. Ela consiste basicamente em verificar se a imagem base se encontra na m\u00e1quina local (no cache de imagens), caso negativo, o Docker Server vai baixar ela do reposit\u00f3rio. Cada comando RUN consiste de uma nova etapa. No primeiro comando RUN, um container ser\u00e1 criado com a imagem base. Uma vez montado, o comando explicitado ser\u00e1 executado e o sistema de arquivos, que at\u00e9 ent\u00e3o era o da imagem base, ser\u00e1 transformado para um novo estado (seja pela adi\u00e7\u00e3o de novas pastas e arquivos, seja pela instala\u00e7\u00e3o de novos pacotes). Ao final, uma nova imagem ser\u00e1 criada, dessa vez contendo o atual estado do sistema de arquivos. No segundo comando RUN o mesmo processo vai ocorrer. A \u00fanica diferen\u00e7a \u00e9 que o container vai ser criado a partir da imagem que foi gerada na etapa imediatamente anterior! (descrita no par\u00e1grafo acima). Ap\u00f3s todos os comandos RUN, a imagem resultante do processo (retornada ap\u00f3s a execu\u00e7\u00e3o do \u00faltimo comando RUN) vai ser configurada com o comando de execu\u00e7\u00e3o do processo principal.","title":"Section 3 - Building custom images"},{"location":"udemy/docker/cap04-making-real-projects/","text":"Section 4 - Making real projects with Docker Lessons 38 - 49 A basic Dockerfile used to run Nodejs applications would be as follows: FROM node:alpine WORKDIR /usr/app COPY ./ ./ RUN npm install CMD [\"npm\",\"start\"] Important Notes: Some frameworks need additional files to build applications. For instance, to set up and run a Nodejs application, two commands are required: >> npm install >> npm start The first command will install all the dependencies required by the application. Those dependencies are specified in a file called package.json . Thus, if you have to run npm install from inside the container, you must first send the package.json file from your local machine to within it. That is what the COPY instruction is for. COPY ./ ./ It will move all files from the context (the directory where the Dockerfile is located) to inside the container. You can also specify what files you want to copy. Attention! It is not best practice moving application files to the root directory of the container's file system. You may end up causing conflicts with the files and folders of the system. To prevent this, you can specify a path to a working directory inside the container. That way, \"./\" will refer to this path, and all files will be placed there. WORKDIR /usr/app Lesson 50 There is a performance issue with the dockerfile presented before. The process of installing dependencies usually takes a lot of time. If you take a closer look at the following lines: COPY ./ ./ RUN npm install You may notice that if one makes a little change to the source code of the application, the Docker Server, before starting to execute the command COPY, will realize that some files were changed, then, it will trigger the re-execution of this line and every other that follows. So, even though there were no changes in the dependencies, the process will repeat. To make the proccess more efficient, it is best practice to install the dependencies first. A better version of the dockerfile would be as presented bellow. FROM node:alpine WORKDIR /usr/app # Install dependencies COPY ./package.json ./ RUN npm install # Copy everything else COPY ./ ./ CMD [\"npm\",\"start\"]","title":"**Section 4 - Making real projects with Docker**"},{"location":"udemy/docker/cap04-making-real-projects/#section-4-making-real-projects-with-docker","text":"","title":"Section 4 - Making real projects with Docker"},{"location":"udemy/docker/cap04-making-real-projects/#lessons-38-49","text":"A basic Dockerfile used to run Nodejs applications would be as follows: FROM node:alpine WORKDIR /usr/app COPY ./ ./ RUN npm install CMD [\"npm\",\"start\"]","title":"Lessons 38 - 49"},{"location":"udemy/docker/cap04-making-real-projects/#important-notes","text":"Some frameworks need additional files to build applications. For instance, to set up and run a Nodejs application, two commands are required: >> npm install >> npm start The first command will install all the dependencies required by the application. Those dependencies are specified in a file called package.json . Thus, if you have to run npm install from inside the container, you must first send the package.json file from your local machine to within it. That is what the COPY instruction is for. COPY ./ ./ It will move all files from the context (the directory where the Dockerfile is located) to inside the container. You can also specify what files you want to copy. Attention! It is not best practice moving application files to the root directory of the container's file system. You may end up causing conflicts with the files and folders of the system. To prevent this, you can specify a path to a working directory inside the container. That way, \"./\" will refer to this path, and all files will be placed there. WORKDIR /usr/app","title":"Important Notes:"},{"location":"udemy/docker/cap04-making-real-projects/#lesson-50","text":"There is a performance issue with the dockerfile presented before. The process of installing dependencies usually takes a lot of time. If you take a closer look at the following lines: COPY ./ ./ RUN npm install You may notice that if one makes a little change to the source code of the application, the Docker Server, before starting to execute the command COPY, will realize that some files were changed, then, it will trigger the re-execution of this line and every other that follows. So, even though there were no changes in the dependencies, the process will repeat. To make the proccess more efficient, it is best practice to install the dependencies first. A better version of the dockerfile would be as presented bellow. FROM node:alpine WORKDIR /usr/app # Install dependencies COPY ./package.json ./ RUN npm install # Copy everything else COPY ./ ./ CMD [\"npm\",\"start\"]","title":"Lesson 50"},{"location":"udemy/docker/cap05-docker-compose/","text":"Section 5 - Docker compose with multiple local containers A docker-compose file is the best way of managing multiple containers in a local machine. Until now, you have been working with containers using the CLI, which is the imperative way of dealing with docker. docker-compose offers a declarative way of managing container infrastructure. You just specify what you want to do and the Docker Server will do all the work to deploy it. Example: docker-compose.yml # Docker-compose version version: '3' # Begin services array services: # Name of the first service redis-server: # Name of the image that is going to be used to create containers for this service image: 'redis' # Name of the second service node-app: # In this case, the image is going to be built from a dockerfile # The dockerfile is located at the actual directory - \".\" build: . # The container will be deployed with the following port mapping ports: - \"8081:8081\" # No matter the reason, the container will be restarted as soon as it gets stopped restart: always Important notes about some fields: restart: always There are other options for this field. Lets check each one of them: always as already stated, the container will be restarted as soon as it gets stopped, no matter the reason. no the container will never be restarted. If it gets stopped, it will remain as such. on-failure every process returns an integer when it gets to an end. if the returned value is 0, so the end has been caused on purpose. else, the end has been caused due to an error. for this policy, if a container terminates with an exit code other than zero, then it is going to be restarted. unless-stopped the container will always be restarted, unless it has been stopped by command line.","title":"**Section 5 - Docker compose with multiple local containers**"},{"location":"udemy/docker/cap05-docker-compose/#section-5-docker-compose-with-multiple-local-containers","text":"A docker-compose file is the best way of managing multiple containers in a local machine. Until now, you have been working with containers using the CLI, which is the imperative way of dealing with docker. docker-compose offers a declarative way of managing container infrastructure. You just specify what you want to do and the Docker Server will do all the work to deploy it. Example: docker-compose.yml # Docker-compose version version: '3' # Begin services array services: # Name of the first service redis-server: # Name of the image that is going to be used to create containers for this service image: 'redis' # Name of the second service node-app: # In this case, the image is going to be built from a dockerfile # The dockerfile is located at the actual directory - \".\" build: . # The container will be deployed with the following port mapping ports: - \"8081:8081\" # No matter the reason, the container will be restarted as soon as it gets stopped restart: always","title":"Section 5 - Docker compose with multiple local containers"},{"location":"udemy/docker/cap05-docker-compose/#important-notes-about-some-fields","text":"restart: always There are other options for this field. Lets check each one of them: always as already stated, the container will be restarted as soon as it gets stopped, no matter the reason. no the container will never be restarted. If it gets stopped, it will remain as such. on-failure every process returns an integer when it gets to an end. if the returned value is 0, so the end has been caused on purpose. else, the end has been caused due to an error. for this policy, if a container terminates with an exit code other than zero, then it is going to be restarted. unless-stopped the container will always be restarted, unless it has been stopped by command line.","title":"Important notes about some fields:"},{"location":"udemy/docker/cap06-production-grade-workflow/","text":"Section 6 - Creating a production grade workflow In a professional workflow, there usually exist two dockerfiles, one for development purposes and the other for production ones. Why? Because, in general, when an application is being developed, everything happens inside a development environment. This environment allows the developer to make code changes, test them, and see their effects in real-time. So, no build process is needed to check if the changes have worked well or not. Development To start a development environment in a Nodejs application, one should run the following command: >> npm run start In the code block bellow, you can see an example of a dockerfile that can be used to start a development environment. Dockerfile.dev FROM node:alpine WORKDIR /usr/app COPY ./package.json ./ RUN npm install COPY ./ ./ CMD [\"npm\",\"run\",\"start\"] WARNING !! With the dockerfile above, one can build an image and create a container with a development environment running in its inside. The problem is the following: the container's filesystem was created and filled with all application files in the state they were at the exact moment in which the build command was executed. Once the container is running, you may want to make changes in the code and see its effects on the application. The only problem is that the files you are changing now are not the same as those that have been placed inside the container (during the build process). Docker provides a way of mounting a filesystem of your local machine into the container. That way, every change in the files inside the container will be reflected outside of it and vice-versa. This feature is called Docker Volumes . In the code block below, you can see an example of it. docker-compose.yml # Docker-compose version version: '3' # Begin services array services: web: build: context: . dockerfile: ./Dockerfile.dev ports: \"3000:3000\" volumes: - /app/node_modules - .:./app Two volumes are being declared for the container. - /app/node_modules - .:./app The second one is exactly what we have discussed in the previous paragraph. The current path (or context) is being mounted inside the container (in the path ./app, to be precise). So, the content in the context must match exactly the content inside the container. Therefore, files inside the container that does not exist in the context will be deleted, unless they are bookmarked , which is exactly what the first volume declaration does! During the build process, dependencies have been installed in the image file system. For a Nodejs application, these dependencies are stored in a folder called node_modules. When you deploy a container using the image and mount your local file system inside the container, the docker server will notice that there is a folder called node_modules inside the container that does not match any folder in the context! To avoid it from being deleted, we transform it into a volume for the container. Testing After the changes have been made, it is time to test the app!. There are three different approaches to this situation, each one with its pros and cons. First: : If you have an image with an application installed within it and you just want to check if the tests are all passing with no failures, so you can just run the command: >> docker run -it imageName npm run test With this approach, you won't be able to modify the application code, neither the tests suite. Second : If you have a docker-compose file like the one presented in the Development section of this README, you can first deploy the container in your machine and run the docker exec command to start the tests suit.. >> docker-compose up >> docker exec -it containerID npm run test This approach is pretty handy. The downside of it is that you will have to type the command docker exec whenever you want to run the tests. Third : With this approach, the idea is to create a new service in the docker-compose file responsible for testing the app. It would look like as presented below: tests: build: context: . dockerfile: ./Dockerfile.dev volumes: - /app/node_modules - .:./app command: [\"npm\",\"run\",\"test\"] Now, you can just run docker-compose up to run the tests. There is only one downside to this approach: if the testing framework has some interactive menu to allow the dev to customize the tests suite, it will not be possible to interact with it. The reason is that the process responsible for listening to these inputs (the testing framework) is not the main process running inside the container (which is the npm process), so it is not possible to attach or to execute commands via CLI in a process other than the one which corresponds to the PID 0.","title":"**Section 6 - Creating a production grade workflow**"},{"location":"udemy/docker/cap06-production-grade-workflow/#section-6-creating-a-production-grade-workflow","text":"In a professional workflow, there usually exist two dockerfiles, one for development purposes and the other for production ones. Why? Because, in general, when an application is being developed, everything happens inside a development environment. This environment allows the developer to make code changes, test them, and see their effects in real-time. So, no build process is needed to check if the changes have worked well or not.","title":"Section 6 - Creating a production grade workflow"},{"location":"udemy/docker/cap06-production-grade-workflow/#development","text":"To start a development environment in a Nodejs application, one should run the following command: >> npm run start In the code block bellow, you can see an example of a dockerfile that can be used to start a development environment. Dockerfile.dev FROM node:alpine WORKDIR /usr/app COPY ./package.json ./ RUN npm install COPY ./ ./ CMD [\"npm\",\"run\",\"start\"] WARNING !! With the dockerfile above, one can build an image and create a container with a development environment running in its inside. The problem is the following: the container's filesystem was created and filled with all application files in the state they were at the exact moment in which the build command was executed. Once the container is running, you may want to make changes in the code and see its effects on the application. The only problem is that the files you are changing now are not the same as those that have been placed inside the container (during the build process). Docker provides a way of mounting a filesystem of your local machine into the container. That way, every change in the files inside the container will be reflected outside of it and vice-versa. This feature is called Docker Volumes . In the code block below, you can see an example of it. docker-compose.yml # Docker-compose version version: '3' # Begin services array services: web: build: context: . dockerfile: ./Dockerfile.dev ports: \"3000:3000\" volumes: - /app/node_modules - .:./app Two volumes are being declared for the container. - /app/node_modules - .:./app The second one is exactly what we have discussed in the previous paragraph. The current path (or context) is being mounted inside the container (in the path ./app, to be precise). So, the content in the context must match exactly the content inside the container. Therefore, files inside the container that does not exist in the context will be deleted, unless they are bookmarked , which is exactly what the first volume declaration does! During the build process, dependencies have been installed in the image file system. For a Nodejs application, these dependencies are stored in a folder called node_modules. When you deploy a container using the image and mount your local file system inside the container, the docker server will notice that there is a folder called node_modules inside the container that does not match any folder in the context! To avoid it from being deleted, we transform it into a volume for the container.","title":"Development"},{"location":"udemy/docker/cap06-production-grade-workflow/#testing","text":"After the changes have been made, it is time to test the app!. There are three different approaches to this situation, each one with its pros and cons. First: : If you have an image with an application installed within it and you just want to check if the tests are all passing with no failures, so you can just run the command: >> docker run -it imageName npm run test With this approach, you won't be able to modify the application code, neither the tests suite. Second : If you have a docker-compose file like the one presented in the Development section of this README, you can first deploy the container in your machine and run the docker exec command to start the tests suit.. >> docker-compose up >> docker exec -it containerID npm run test This approach is pretty handy. The downside of it is that you will have to type the command docker exec whenever you want to run the tests. Third : With this approach, the idea is to create a new service in the docker-compose file responsible for testing the app. It would look like as presented below: tests: build: context: . dockerfile: ./Dockerfile.dev volumes: - /app/node_modules - .:./app command: [\"npm\",\"run\",\"test\"] Now, you can just run docker-compose up to run the tests. There is only one downside to this approach: if the testing framework has some interactive menu to allow the dev to customize the tests suite, it will not be possible to interact with it. The reason is that the process responsible for listening to these inputs (the testing framework) is not the main process running inside the container (which is the npm process), so it is not possible to attach or to execute commands via CLI in a process other than the one which corresponds to the PID 0.","title":"Testing"},{"location":"udemy/docker/cap07-continuous-integration-and-deployment/","text":"Section 7 - Continuous integration and deployment A continuous integration pipeline with Travis CI starts when the user creates a pull-request to a master branch. When this situation occurs, Travis CI pulls all the code and starts the testing routines. The testing results are placed in the code-review section of the pull-request page, along with code reviews provided by team members. After this process is completed, one can perform a merge of the pull-request into the master branch, which, in turn will, trigger a continuous deployment pipeline. How does Travis CI know how to test the application? The repository must have a file called .travis.yml , as showned below. .travis.yml # root privileges is required in all commands that are going to be executed throghout the pipeline! sudo: required # Specify software that has to be present in the runtime created by Travis. services: - docker # commands that must be executed to prepare the environment for whatever we want to do. # As we want to run testing routines that are encapsulated inside a container, we need a container up and running in the first place! # The container must be create from the Dockerfile.dev file. Why? # Because the container created from this file contains all dependencies required by the application, including the testing framework. before_install: - docker build -t gabriel-gaspar/react_app -f Dockerfile.dev # Travis assumes the testing routine ends with a status code 0. # This behavior does not occur in Jest (Nodejs testing framework) # After the end of the first execution, the process hangs in an # interactive menu. So, to disable this menu, one as to pass the option # -e CI=true when executing the run command. script: - docker run -e CI=true gabriel-gaspar/docker-react npm run test # Travis can integrate with different cloud providers. # In this case, we are going to deploy the app on AWS Elastic Beanstalk. # For this, you have to the following: # - access the AWS Console, # - choose a Region, # - create an Elastic Beanstalk Application # - Set Docker running on 64bit Amazon Linux as Platform. # - Set EC2 as t2.small. # - create an Environment for the Application. deploy: # Name of the platform in which the app will be deployed. provider: elasticbeanstalk # Region choosed when creating the Application region: \"us-west-2\" # Application name app: \"docker\" # Environment Name env: \"Docker-env\" # Elastic Beanstalk creates a bucket in the S3 Bucket Service to store data about Applications created in each Region. # You have to find out the name of the bucket created for the Region you have selected. bucket_name: \"elasticbeanstalk-us-west-2-342525436456\" # The name of the folder created for your Application, inside the bucket. # In general, it has the same name as your Applicaiton. bucket_path: \"docker\" # Specify what branch will trigger the deployment process. on: branch: main # Travis CI needs IAM credentials to acces you AWS account. # So, you have to create an IAM User for it, attach permissions to fully access the Elastic Beanstalk Service, and generate the credentials. # For this user, make sure you don't allow access through the management console. Only programmatic access is needed. # Once you have the credentials, access the Travis CI console and insert them as runtime environment variables. access_key_id: $AWS_ACCESS_KEY secret_access_key: $AWS_SECRET_KEY","title":"**Section 7 - Continuous integration and deployment**"},{"location":"udemy/docker/cap07-continuous-integration-and-deployment/#section-7-continuous-integration-and-deployment","text":"A continuous integration pipeline with Travis CI starts when the user creates a pull-request to a master branch. When this situation occurs, Travis CI pulls all the code and starts the testing routines. The testing results are placed in the code-review section of the pull-request page, along with code reviews provided by team members. After this process is completed, one can perform a merge of the pull-request into the master branch, which, in turn will, trigger a continuous deployment pipeline. How does Travis CI know how to test the application? The repository must have a file called .travis.yml , as showned below. .travis.yml # root privileges is required in all commands that are going to be executed throghout the pipeline! sudo: required # Specify software that has to be present in the runtime created by Travis. services: - docker # commands that must be executed to prepare the environment for whatever we want to do. # As we want to run testing routines that are encapsulated inside a container, we need a container up and running in the first place! # The container must be create from the Dockerfile.dev file. Why? # Because the container created from this file contains all dependencies required by the application, including the testing framework. before_install: - docker build -t gabriel-gaspar/react_app -f Dockerfile.dev # Travis assumes the testing routine ends with a status code 0. # This behavior does not occur in Jest (Nodejs testing framework) # After the end of the first execution, the process hangs in an # interactive menu. So, to disable this menu, one as to pass the option # -e CI=true when executing the run command. script: - docker run -e CI=true gabriel-gaspar/docker-react npm run test # Travis can integrate with different cloud providers. # In this case, we are going to deploy the app on AWS Elastic Beanstalk. # For this, you have to the following: # - access the AWS Console, # - choose a Region, # - create an Elastic Beanstalk Application # - Set Docker running on 64bit Amazon Linux as Platform. # - Set EC2 as t2.small. # - create an Environment for the Application. deploy: # Name of the platform in which the app will be deployed. provider: elasticbeanstalk # Region choosed when creating the Application region: \"us-west-2\" # Application name app: \"docker\" # Environment Name env: \"Docker-env\" # Elastic Beanstalk creates a bucket in the S3 Bucket Service to store data about Applications created in each Region. # You have to find out the name of the bucket created for the Region you have selected. bucket_name: \"elasticbeanstalk-us-west-2-342525436456\" # The name of the folder created for your Application, inside the bucket. # In general, it has the same name as your Applicaiton. bucket_path: \"docker\" # Specify what branch will trigger the deployment process. on: branch: main # Travis CI needs IAM credentials to acces you AWS account. # So, you have to create an IAM User for it, attach permissions to fully access the Elastic Beanstalk Service, and generate the credentials. # For this user, make sure you don't allow access through the management console. Only programmatic access is needed. # Once you have the credentials, access the Travis CI console and insert them as runtime environment variables. access_key_id: $AWS_ACCESS_KEY secret_access_key: $AWS_SECRET_KEY","title":"Section 7 - Continuous integration and deployment"},{"location":"udemy/docker/cap08-cap09-building-multi-container-app-and-dockerizing-it/","text":"Sections 8 and 9 - Building multi-container applications and dockerizing them In this section, we will learn how to set up a multi-container development environment using docker-compose. The process consists of the following steps: create a Dockerfile.dev within each project folder build the image for each project create a docker-compose.yml file do deploy the containers create a nginx server to proxy incoming requests based on route params. The most important thing here is to understand the very last step, that is the nginx server. The nginx server will proxy incoming requests based on route params. For instance, if the developer enters an URL ending with /api/values/current, so the request will be redirected to the backend. If the URL ends with \"/\", it will be redirected to the frontend application. In the host machine, the nginx server knows exactly on which port each service is listening to (we configure this through rules written in a default.conf file). Why should we do that? Because otherwise the developer would have to know the ports of each service, furthermore, it would have to be hardcoded in the source code of the application which would be terribly bad, once ports are very volatile resources. It is also good practice to remove the '/api/\" from the request forwarded by the nginx server, once its only intent is for routing. A nginx default.conf file would be as follows: default.conf # Declare an upstream server, named client upstream client{ # the upstream has a server running on the URL: client:3000 # in this url, \"client\" is the name given to the service in the docker-compose file. server client:3000; } # Declare an upstream server, named server upstream server{ # the upstream has a server running on the URL: server:3000 # in this url, \"server\" is the name given to the service in the docker-compose file. server server:5000; } # Configure the nginx server server { # It will listen on the port 80 of the host machine listen 80; # First routing rule # If the request path is \"/\" location / { # Redirect it to the URL below. # \"client\" is one of the upstream services create above. proxy_pass http://client; } # Second routing rule # If the request path is \"/api\" location /api { # Chops the incoming request path so that the forwarded request does not contains the \"/api\" prefix. rewrite /api/(.*) /$1 break; # Redirect it to the URL below. # \"server\" is one of the upstream services create above proxy_pass http://server; } # Allow websocket connections # For this class, the front-end application needs websocket connections with the backend. # So, as the nginx is in the way, it has to allow such connections through it. location /sockjs-node { proxy_pass http://client; proxy_http_version 1.1; pxoy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; } }","title":"**Sections 8 and 9 - Building multi-container applications and dockerizing them **"},{"location":"udemy/docker/cap08-cap09-building-multi-container-app-and-dockerizing-it/#sections-8-and-9-building-multi-container-applications-and-dockerizing-them","text":"In this section, we will learn how to set up a multi-container development environment using docker-compose. The process consists of the following steps: create a Dockerfile.dev within each project folder build the image for each project create a docker-compose.yml file do deploy the containers create a nginx server to proxy incoming requests based on route params. The most important thing here is to understand the very last step, that is the nginx server. The nginx server will proxy incoming requests based on route params. For instance, if the developer enters an URL ending with /api/values/current, so the request will be redirected to the backend. If the URL ends with \"/\", it will be redirected to the frontend application. In the host machine, the nginx server knows exactly on which port each service is listening to (we configure this through rules written in a default.conf file). Why should we do that? Because otherwise the developer would have to know the ports of each service, furthermore, it would have to be hardcoded in the source code of the application which would be terribly bad, once ports are very volatile resources. It is also good practice to remove the '/api/\" from the request forwarded by the nginx server, once its only intent is for routing. A nginx default.conf file would be as follows: default.conf # Declare an upstream server, named client upstream client{ # the upstream has a server running on the URL: client:3000 # in this url, \"client\" is the name given to the service in the docker-compose file. server client:3000; } # Declare an upstream server, named server upstream server{ # the upstream has a server running on the URL: server:3000 # in this url, \"server\" is the name given to the service in the docker-compose file. server server:5000; } # Configure the nginx server server { # It will listen on the port 80 of the host machine listen 80; # First routing rule # If the request path is \"/\" location / { # Redirect it to the URL below. # \"client\" is one of the upstream services create above. proxy_pass http://client; } # Second routing rule # If the request path is \"/api\" location /api { # Chops the incoming request path so that the forwarded request does not contains the \"/api\" prefix. rewrite /api/(.*) /$1 break; # Redirect it to the URL below. # \"server\" is one of the upstream services create above proxy_pass http://server; } # Allow websocket connections # For this class, the front-end application needs websocket connections with the backend. # So, as the nginx is in the way, it has to allow such connections through it. location /sockjs-node { proxy_pass http://client; proxy_http_version 1.1; pxoy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; } }","title":"Sections 8 and 9 - Building multi-container applications and dockerizing them"},{"location":"udemy/docker/cap08-cap09-building-multi-container-app-and-dockerizing-it/client/","text":"This project was bootstrapped with Create React App . Available Scripts In the project directory, you can run: npm start Runs the app in the development mode. Open http://localhost:3000 to view it in the browser. The page will reload if you make edits. You will also see any lint errors in the console. npm test Launches the test runner in the interactive watch mode. See the section about running tests for more information. npm run build Builds the app for production to the build folder. It correctly bundles React in production mode and optimizes the build for the best performance. The build is minified and the filenames include the hashes. Your app is ready to be deployed! See the section about deployment for more information. npm run eject Note: this is a one-way operation. Once you eject , you can\u2019t go back! If you aren\u2019t satisfied with the build tool and configuration choices, you can eject at any time. This command will remove the single build dependency from your project. Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except eject will still work, but they will point to the copied scripts so you can tweak them. At this point you\u2019re on your own. You don\u2019t have to ever use eject . The curated feature set is suitable for small and middle deployments, and you shouldn\u2019t feel obligated to use this feature. However we understand that this tool wouldn\u2019t be useful if you couldn\u2019t customize it when you are ready for it. Learn More You can learn more in the Create React App documentation . To learn React, check out the React documentation . Code Splitting This section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting Analyzing the Bundle Size This section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size Making a Progressive Web App This section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app Advanced Configuration This section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration Deployment This section has moved here: https://facebook.github.io/create-react-app/docs/deployment npm run build fails to minify This section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify","title":"Index"},{"location":"udemy/docker/cap08-cap09-building-multi-container-app-and-dockerizing-it/client/#available-scripts","text":"In the project directory, you can run:","title":"Available Scripts"},{"location":"udemy/docker/cap08-cap09-building-multi-container-app-and-dockerizing-it/client/#npm-start","text":"Runs the app in the development mode. Open http://localhost:3000 to view it in the browser. The page will reload if you make edits. You will also see any lint errors in the console.","title":"npm start"},{"location":"udemy/docker/cap08-cap09-building-multi-container-app-and-dockerizing-it/client/#npm-test","text":"Launches the test runner in the interactive watch mode. See the section about running tests for more information.","title":"npm test"},{"location":"udemy/docker/cap08-cap09-building-multi-container-app-and-dockerizing-it/client/#npm-run-build","text":"Builds the app for production to the build folder. It correctly bundles React in production mode and optimizes the build for the best performance. The build is minified and the filenames include the hashes. Your app is ready to be deployed! See the section about deployment for more information.","title":"npm run build"},{"location":"udemy/docker/cap08-cap09-building-multi-container-app-and-dockerizing-it/client/#npm-run-eject","text":"Note: this is a one-way operation. Once you eject , you can\u2019t go back! If you aren\u2019t satisfied with the build tool and configuration choices, you can eject at any time. This command will remove the single build dependency from your project. Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except eject will still work, but they will point to the copied scripts so you can tweak them. At this point you\u2019re on your own. You don\u2019t have to ever use eject . The curated feature set is suitable for small and middle deployments, and you shouldn\u2019t feel obligated to use this feature. However we understand that this tool wouldn\u2019t be useful if you couldn\u2019t customize it when you are ready for it.","title":"npm run eject"},{"location":"udemy/docker/cap08-cap09-building-multi-container-app-and-dockerizing-it/client/#learn-more","text":"You can learn more in the Create React App documentation . To learn React, check out the React documentation .","title":"Learn More"},{"location":"udemy/docker/cap08-cap09-building-multi-container-app-and-dockerizing-it/client/#code-splitting","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting","title":"Code Splitting"},{"location":"udemy/docker/cap08-cap09-building-multi-container-app-and-dockerizing-it/client/#analyzing-the-bundle-size","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size","title":"Analyzing the Bundle Size"},{"location":"udemy/docker/cap08-cap09-building-multi-container-app-and-dockerizing-it/client/#making-a-progressive-web-app","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app","title":"Making a Progressive Web App"},{"location":"udemy/docker/cap08-cap09-building-multi-container-app-and-dockerizing-it/client/#advanced-configuration","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration","title":"Advanced Configuration"},{"location":"udemy/docker/cap08-cap09-building-multi-container-app-and-dockerizing-it/client/#deployment","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/deployment","title":"Deployment"},{"location":"udemy/docker/cap08-cap09-building-multi-container-app-and-dockerizing-it/client/#npm-run-build-fails-to-minify","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify","title":"npm run build fails to minify"},{"location":"udemy/docker/cap10-continuous-integration-for-multiple-images/","text":"Section 10 - A continuous integration workflow for multiple images The workflow consists of the following steps: Push code to github Travis automatically pulls the repository Travis builds a test image and tests the code Once the tests have successfuly passed, Travis builds prod images Travis pushes the prod images to the Docker Hub Travis pushes a notification to Elastic Beanstalk Elastic Beanstalk pulls the images from Docker Hub and deploys them. The first step is to create a prod version of the dockerfile for each project. Even though some dockerfiles end up being the same, it is best practice to have two versions of it anyway, one for development purposes and another for production. In the client (frontend) application, it is important to take into consideration the following: In the development environment, the frontend is a React Server that establishes a websocket connection with client browsers to refresh them automatically when changes are made in code. On the other hand, in the production environment, the React application is only a bunch of static files to be served by a Webserver. In this case, the webserver is going to be another nginx server which will have a slightly different configuration than the one we have configured to proxy requests between backends.","title":"**Section 10 - A continuous integration workflow for multiple images**"},{"location":"udemy/docker/cap10-continuous-integration-for-multiple-images/#section-10-a-continuous-integration-workflow-for-multiple-images","text":"The workflow consists of the following steps: Push code to github Travis automatically pulls the repository Travis builds a test image and tests the code Once the tests have successfuly passed, Travis builds prod images Travis pushes the prod images to the Docker Hub Travis pushes a notification to Elastic Beanstalk Elastic Beanstalk pulls the images from Docker Hub and deploys them. The first step is to create a prod version of the dockerfile for each project. Even though some dockerfiles end up being the same, it is best practice to have two versions of it anyway, one for development purposes and another for production. In the client (frontend) application, it is important to take into consideration the following: In the development environment, the frontend is a React Server that establishes a websocket connection with client browsers to refresh them automatically when changes are made in code. On the other hand, in the production environment, the React application is only a bunch of static files to be served by a Webserver. In this case, the webserver is going to be another nginx server which will have a slightly different configuration than the one we have configured to proxy requests between backends.","title":"Section 10 - A continuous integration workflow for multiple images"},{"location":"udemy/docker/cap10-continuous-integration-for-multiple-images/client/","text":"This project was bootstrapped with Create React App . Available Scripts In the project directory, you can run: npm start Runs the app in the development mode. Open http://localhost:3000 to view it in the browser. The page will reload if you make edits. You will also see any lint errors in the console. npm test Launches the test runner in the interactive watch mode. See the section about running tests for more information. npm run build Builds the app for production to the build folder. It correctly bundles React in production mode and optimizes the build for the best performance. The build is minified and the filenames include the hashes. Your app is ready to be deployed! See the section about deployment for more information. npm run eject Note: this is a one-way operation. Once you eject , you can\u2019t go back! If you aren\u2019t satisfied with the build tool and configuration choices, you can eject at any time. This command will remove the single build dependency from your project. Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except eject will still work, but they will point to the copied scripts so you can tweak them. At this point you\u2019re on your own. You don\u2019t have to ever use eject . The curated feature set is suitable for small and middle deployments, and you shouldn\u2019t feel obligated to use this feature. However we understand that this tool wouldn\u2019t be useful if you couldn\u2019t customize it when you are ready for it. Learn More You can learn more in the Create React App documentation . To learn React, check out the React documentation . Code Splitting This section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting Analyzing the Bundle Size This section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size Making a Progressive Web App This section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app Advanced Configuration This section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration Deployment This section has moved here: https://facebook.github.io/create-react-app/docs/deployment npm run build fails to minify This section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify","title":"Index"},{"location":"udemy/docker/cap10-continuous-integration-for-multiple-images/client/#available-scripts","text":"In the project directory, you can run:","title":"Available Scripts"},{"location":"udemy/docker/cap10-continuous-integration-for-multiple-images/client/#npm-start","text":"Runs the app in the development mode. Open http://localhost:3000 to view it in the browser. The page will reload if you make edits. You will also see any lint errors in the console.","title":"npm start"},{"location":"udemy/docker/cap10-continuous-integration-for-multiple-images/client/#npm-test","text":"Launches the test runner in the interactive watch mode. See the section about running tests for more information.","title":"npm test"},{"location":"udemy/docker/cap10-continuous-integration-for-multiple-images/client/#npm-run-build","text":"Builds the app for production to the build folder. It correctly bundles React in production mode and optimizes the build for the best performance. The build is minified and the filenames include the hashes. Your app is ready to be deployed! See the section about deployment for more information.","title":"npm run build"},{"location":"udemy/docker/cap10-continuous-integration-for-multiple-images/client/#npm-run-eject","text":"Note: this is a one-way operation. Once you eject , you can\u2019t go back! If you aren\u2019t satisfied with the build tool and configuration choices, you can eject at any time. This command will remove the single build dependency from your project. Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except eject will still work, but they will point to the copied scripts so you can tweak them. At this point you\u2019re on your own. You don\u2019t have to ever use eject . The curated feature set is suitable for small and middle deployments, and you shouldn\u2019t feel obligated to use this feature. However we understand that this tool wouldn\u2019t be useful if you couldn\u2019t customize it when you are ready for it.","title":"npm run eject"},{"location":"udemy/docker/cap10-continuous-integration-for-multiple-images/client/#learn-more","text":"You can learn more in the Create React App documentation . To learn React, check out the React documentation .","title":"Learn More"},{"location":"udemy/docker/cap10-continuous-integration-for-multiple-images/client/#code-splitting","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting","title":"Code Splitting"},{"location":"udemy/docker/cap10-continuous-integration-for-multiple-images/client/#analyzing-the-bundle-size","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size","title":"Analyzing the Bundle Size"},{"location":"udemy/docker/cap10-continuous-integration-for-multiple-images/client/#making-a-progressive-web-app","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app","title":"Making a Progressive Web App"},{"location":"udemy/docker/cap10-continuous-integration-for-multiple-images/client/#advanced-configuration","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration","title":"Advanced Configuration"},{"location":"udemy/docker/cap10-continuous-integration-for-multiple-images/client/#deployment","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/deployment","title":"Deployment"},{"location":"udemy/docker/cap10-continuous-integration-for-multiple-images/client/#npm-run-build-fails-to-minify","text":"This section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify","title":"npm run build fails to minify"},{"location":"udemy/linux/cap03-system-access-and-file-system/","text":"Section 3 - System Access and File System Lesson 33 To access a Linux machine remotely from a Windows machine, a software called Putty is needed. If both machines are Linux, the access can be made via SSH connection and no third-party package is required. Lesson 35 To connect to a remote machine, its IP address must be known. It is possible to figure it out by accessing its terminal and listing its network interfaces. The commands to do that are listed below: >> ifconfig >> ip addr >> ip a The ifconfig command is being deprecated in the latest Linux versions. So, you would rather prefer the other two options. Lesson 36 If the remote Linux machine does not have an IP address but does have a network interface, then you will need to activate it manually. To do that, access the terminal as root and run the following command: >> ifup enp0s3 This command will activate the network interface, which in turn will receive an IP address. In the case above, the network interface was identified by enp0s3 . Lessons 38-39 The Linux file system is like a directories tree organized hierarchically. At the top, there is a directory called Root. It is also referred to as Slash. Within that folder, there is a series of subfolders responsible for storing files and data for different purposes. /boot when the system starts, this is the first directory accessed. files with information about the boot of the system are stored here. /root this is the root user directory /dev it stores files containing information about all physical media of the machine, like HD, audio, USB, keyboard, etc. /etc it stores configuration files for all native Linux applications, like FTP, NFS, e-mail, DNS, etc. if you intend to edit any of them, make a snapshot of it in advance! /bin ou /usr/b in it stores files containing the definition of the commands a user can invoke through the terminal (ls, pwd, cd, etc...) /sbin ou /usr/sbin stores files containing configuration data of the Linux file system. /opt It stores all files related to the third-party applications the user installs. /proc each process active in the machine has a file in this folder. when the user shuts down the system, all files are deleted. /lib ou /usr/lib it contains C libraries used by applications installed on the system. if the user installs an application that requires some C scripts, those scrips are stored in this folder. /tmp folder meant to store temporary files. /home every user of the system has a folder in this directory. these folders represent the working directory of each user of the system. /var every log issued by either the system or the applications running on it is stored here. /run it stores information about processes running on the system, like PID files. when the user shuts down the system, all files here are deleted. /mnt file-systems mounted in the Linux are placed here. /media directory in which the data related to every external media is located, like CD-ROM and others. Lesson 42 In a Linux system, everything is a file. There are seven types of files. regular file directory link special file or device file socket named pipe block device In a directory, you can run the following command to list all files inside of it, each one with its corresponding type. >> ls -l If the file is a directory, the type field will start by the character d . Example: dr-xr-xr-x If the file is a link, the type field will start by the character l . Example: lrwxrwxrwt If the file is a regular one, the type field will start by the character - . Example: -rw-r--r-- Lesson 46 There are two commands available to search for files in a Linux system: locate and find . The big difference between them is that the first one (locate) maintains a database (a cache) with the path to all files in the system. Thus, when the command is executed, the file is searched in the data base, which returns a quick response (it is faster than the find command). The second one ( find ) is just a simple command that loops across all files in the system. Another important note about the locate command: from time to time, the system runs the command updatedb to update the cache. The package mlocate is required to start using the locate instruction. You can install it with the command: >> yum install mlocate Lesson 48 In certain situations, you will have to operate on a large amount of files (creation, deletion, copy, etc.). If the file names follow a certain pattern, you can make use of a feature called WildCards to operate in each one that matches the pattern. There are basically 3 Wildcards: * represents the whole (totality) ? represents a value which value does not matter [] represents a set o selected values Example1: Let's create 9 files whose names follow the pattern: file1-xyz, file2-xyz, file3-xyz... To do that, we can use the wildcard that represents a set of selected values. >> touch file[1..9]-xyz Example2: Let's list every file that starts with \"abc\" and ends with whatever set of characters. In this case, we can use the wildcard that represents the totality. >> ls -l abc* Example3: Let's list every file that ends with \"xyz\". >> ls -l *xyz Example4: Let's list every file that starts with any character, and followed by \"bcd\". In this case, we have to combine two wildcards. >> ls -l ?bcd* Example5: Let's list every file that contains the string \"xy\". It does not matter which characters come first and which characters come after the string. Furthermore, it does not even matter the amount of them. That is another case for the totality wildcard. >> ls -l *xy* Example6: Let's list all files that contains or the character \"x\", or the character \"y\". >> ls -l *[xy]* Lesson 49 Links are a resource very similar to shortcuts in Windows systems. They can come in two forms: Hard e Soft . Every file in a Linux system receives an identifier. This number, known as inode , refers to the address in memory where the file is stored. A hard link is a \"file\" that actualy is a pointer to an inode . So, if you create a hard link to a file (or folder), you are creating a pointer that is pointing to the address in memory where the original file is located. An important note: if the name of the original file changes, nothing will happen to the hard-link. It will continue pointing to the same file, once the inode remains the same. A soft link is slightly different. It is a pointer to the path of a file. So, if the name of this file changes, the link breaks, because the path to the refered file will change. If you want to check what are the inodes of the files in a given directory, just run the following command: >> ls -ltri Examples : Suppose that we have a file in the following path: /home/gaspar/contract Let's create a hard-link and a soft-link to it. >> ln /home/gaspar/contract >> ln -s /home/gaspar/contract","title":"**Section 3 - System Access and File System**"},{"location":"udemy/linux/cap03-system-access-and-file-system/#section-3-system-access-and-file-system","text":"","title":"Section 3 - System Access and File System"},{"location":"udemy/linux/cap03-system-access-and-file-system/#lesson-33","text":"To access a Linux machine remotely from a Windows machine, a software called Putty is needed. If both machines are Linux, the access can be made via SSH connection and no third-party package is required.","title":"Lesson 33"},{"location":"udemy/linux/cap03-system-access-and-file-system/#lesson-35","text":"To connect to a remote machine, its IP address must be known. It is possible to figure it out by accessing its terminal and listing its network interfaces. The commands to do that are listed below: >> ifconfig >> ip addr >> ip a The ifconfig command is being deprecated in the latest Linux versions. So, you would rather prefer the other two options.","title":"Lesson 35"},{"location":"udemy/linux/cap03-system-access-and-file-system/#lesson-36","text":"If the remote Linux machine does not have an IP address but does have a network interface, then you will need to activate it manually. To do that, access the terminal as root and run the following command: >> ifup enp0s3 This command will activate the network interface, which in turn will receive an IP address. In the case above, the network interface was identified by enp0s3 .","title":"Lesson 36"},{"location":"udemy/linux/cap03-system-access-and-file-system/#lessons-38-39","text":"The Linux file system is like a directories tree organized hierarchically. At the top, there is a directory called Root. It is also referred to as Slash. Within that folder, there is a series of subfolders responsible for storing files and data for different purposes. /boot when the system starts, this is the first directory accessed. files with information about the boot of the system are stored here. /root this is the root user directory /dev it stores files containing information about all physical media of the machine, like HD, audio, USB, keyboard, etc. /etc it stores configuration files for all native Linux applications, like FTP, NFS, e-mail, DNS, etc. if you intend to edit any of them, make a snapshot of it in advance! /bin ou /usr/b in it stores files containing the definition of the commands a user can invoke through the terminal (ls, pwd, cd, etc...) /sbin ou /usr/sbin stores files containing configuration data of the Linux file system. /opt It stores all files related to the third-party applications the user installs. /proc each process active in the machine has a file in this folder. when the user shuts down the system, all files are deleted. /lib ou /usr/lib it contains C libraries used by applications installed on the system. if the user installs an application that requires some C scripts, those scrips are stored in this folder. /tmp folder meant to store temporary files. /home every user of the system has a folder in this directory. these folders represent the working directory of each user of the system. /var every log issued by either the system or the applications running on it is stored here. /run it stores information about processes running on the system, like PID files. when the user shuts down the system, all files here are deleted. /mnt file-systems mounted in the Linux are placed here. /media directory in which the data related to every external media is located, like CD-ROM and others.","title":"Lessons 38-39"},{"location":"udemy/linux/cap03-system-access-and-file-system/#lesson-42","text":"In a Linux system, everything is a file. There are seven types of files. regular file directory link special file or device file socket named pipe block device In a directory, you can run the following command to list all files inside of it, each one with its corresponding type. >> ls -l If the file is a directory, the type field will start by the character d . Example: dr-xr-xr-x If the file is a link, the type field will start by the character l . Example: lrwxrwxrwt If the file is a regular one, the type field will start by the character - . Example: -rw-r--r--","title":"Lesson 42"},{"location":"udemy/linux/cap03-system-access-and-file-system/#lesson-46","text":"There are two commands available to search for files in a Linux system: locate and find . The big difference between them is that the first one (locate) maintains a database (a cache) with the path to all files in the system. Thus, when the command is executed, the file is searched in the data base, which returns a quick response (it is faster than the find command). The second one ( find ) is just a simple command that loops across all files in the system. Another important note about the locate command: from time to time, the system runs the command updatedb to update the cache. The package mlocate is required to start using the locate instruction. You can install it with the command: >> yum install mlocate","title":"Lesson 46"},{"location":"udemy/linux/cap03-system-access-and-file-system/#lesson-48","text":"In certain situations, you will have to operate on a large amount of files (creation, deletion, copy, etc.). If the file names follow a certain pattern, you can make use of a feature called WildCards to operate in each one that matches the pattern. There are basically 3 Wildcards: * represents the whole (totality) ? represents a value which value does not matter [] represents a set o selected values Example1: Let's create 9 files whose names follow the pattern: file1-xyz, file2-xyz, file3-xyz... To do that, we can use the wildcard that represents a set of selected values. >> touch file[1..9]-xyz Example2: Let's list every file that starts with \"abc\" and ends with whatever set of characters. In this case, we can use the wildcard that represents the totality. >> ls -l abc* Example3: Let's list every file that ends with \"xyz\". >> ls -l *xyz Example4: Let's list every file that starts with any character, and followed by \"bcd\". In this case, we have to combine two wildcards. >> ls -l ?bcd* Example5: Let's list every file that contains the string \"xy\". It does not matter which characters come first and which characters come after the string. Furthermore, it does not even matter the amount of them. That is another case for the totality wildcard. >> ls -l *xy* Example6: Let's list all files that contains or the character \"x\", or the character \"y\". >> ls -l *[xy]*","title":"Lesson 48"},{"location":"udemy/linux/cap03-system-access-and-file-system/#lesson-49","text":"Links are a resource very similar to shortcuts in Windows systems. They can come in two forms: Hard e Soft . Every file in a Linux system receives an identifier. This number, known as inode , refers to the address in memory where the file is stored. A hard link is a \"file\" that actualy is a pointer to an inode . So, if you create a hard link to a file (or folder), you are creating a pointer that is pointing to the address in memory where the original file is located. An important note: if the name of the original file changes, nothing will happen to the hard-link. It will continue pointing to the same file, once the inode remains the same. A soft link is slightly different. It is a pointer to the path of a file. So, if the name of this file changes, the link breaks, because the path to the refered file will change. If you want to check what are the inodes of the files in a given directory, just run the following command: >> ls -ltri Examples : Suppose that we have a file in the following path: /home/gaspar/contract Let's create a hard-link and a soft-link to it. >> ln /home/gaspar/contract >> ln -s /home/gaspar/contract","title":"Lesson 49"},{"location":"udemy/linux/cap04-linux-fundamentals/","text":"Section 4 - Linux Fundamentals Aula 53 Para obter detalhes sobre os comandos do linux, diretamente pelo terminal, use o comando \"man\". Ex: >> man ls Aula 54 Todos os arquivos do linux possuem permiss\u00f5es que s\u00e3o atribu\u00eddas em tr\u00eas n\u00edveis: u - usu\u00e1rios cadastrados no sistema g - grupos de usu\u00e1rios cadastrados no sistema o - outros usu\u00e1rios e convidados Existem tr\u00eas tipos de permiss\u00f5es: - r - leitura (read) - w - escrita (w) - x - execu\u00e7\u00e3o (execute) Ao rodar o comando >> ls -l Ser\u00e3o exibidas as permiss\u00f5es de acesso a cada arquivo dentro do diret\u00f3rio. Ex: Supondo que um arquivo chamado jerry tenha o seguinte tipo: -rw-rw-r-- o primeiro caractere \"-\" indica que o arquivo \u00e9 um arquivo, propriamente dito. os pr\u00f3ximos 3 caracteres indicam as permiss\u00f5es atribu\u00eddas ao usu\u00e1rio dono do arquivo. rw- permiss\u00e3o para leitura e escrita. os pr\u00f3ximos tr\u00eas caracteres indicam as permiss\u00f5es atribu\u00eddas aos grupos de usu\u00e1rios aos quais o arquivo pertence rw- permiss\u00e3o para leitura e escrita os \u00faltimos 3 caracteres indicam as permiss\u00f5es atribu\u00eddas a quaisquer outros usu\u00e1rios do sistema r-- permiss\u00e3o para leitura As permiss\u00f5es podem ser alteradas atrav\u00e9s do comando chmod (change moderators). Curiosidade : Os diret\u00f3rios possuem o campo type no seguinte formato: drwxrwxr-x Se a permiss\u00e3o de execu\u00e7\u00e3o (x) for removida de todos os n\u00edveis (u,g,o), ningu\u00e9m poder\u00e1 acessar a pasta, apenas o usu\u00e1rio root . Aula 56 Arquivos do linux pertencem a usu\u00e1rios e grupos de usu\u00e1rios. Apenas o usu\u00e1rio root pode alterar os donos de um arquivo. O dono de um arquivo, ou o grupo de usu\u00e1rios ao qual um arquivo pertence, podem ser alterados pelos comandos: chown e chgrp Curiosidade: Se voc\u00ea for o usu\u00e1rio gaspar e dentro do seu diret\u00f3rio (/home/gaspar) haver um arquivo que pertence ao usu\u00e1rio root, voc\u00ea ainda assim poder\u00e1 fazer o que quiser com esse arquivo, pois o seu diret\u00f3rio /home/gaspar pertence a voc\u00ea e te permite ler, escrever e executar qualquer arquivo que estiver dentro. Aula 57 Para dar permiss\u00f5es espec\u00edficas para um usu\u00e1rios, ou para um grupo de usu\u00e1rio, sobre um arquivo, deve-se usar o recurso Access List Control. Toda vez que um arquivo possuir permiss\u00f5es desse tipo, um \"+\" ser\u00e1 o ultimo caractere do campo type desse arquivo. Exemplos: Para listar as permiss\u00f5es ACL de um arquivo: >> getfacl path/to/file Para adicionar permiss\u00f5es a um usu\u00e1rio em especifico: >> setfacl -m u:userName:rwx /path/to/file Para adicionar permiss\u00f5es a um grupo de usu\u00e1rios espec\u00edfico: >> setfacl -m g:groupName:rw path/to/file Para adicionar permiss\u00f5es a um diret\u00f3rio e todos os seus arquivos filhos: >> setfacl -dm \"rwx\" /path/to/file Para remover todas as permiss\u00f5es ACL: >> setfacl -b /path/to/file","title":"**Section 4 - Linux Fundamentals**"},{"location":"udemy/linux/cap04-linux-fundamentals/#section-4-linux-fundamentals","text":"","title":"Section 4 - Linux Fundamentals"},{"location":"udemy/linux/cap04-linux-fundamentals/#aula-53","text":"Para obter detalhes sobre os comandos do linux, diretamente pelo terminal, use o comando \"man\". Ex: >> man ls","title":"Aula 53"},{"location":"udemy/linux/cap04-linux-fundamentals/#aula-54","text":"Todos os arquivos do linux possuem permiss\u00f5es que s\u00e3o atribu\u00eddas em tr\u00eas n\u00edveis: u - usu\u00e1rios cadastrados no sistema g - grupos de usu\u00e1rios cadastrados no sistema o - outros usu\u00e1rios e convidados Existem tr\u00eas tipos de permiss\u00f5es: - r - leitura (read) - w - escrita (w) - x - execu\u00e7\u00e3o (execute) Ao rodar o comando >> ls -l Ser\u00e3o exibidas as permiss\u00f5es de acesso a cada arquivo dentro do diret\u00f3rio. Ex: Supondo que um arquivo chamado jerry tenha o seguinte tipo: -rw-rw-r-- o primeiro caractere \"-\" indica que o arquivo \u00e9 um arquivo, propriamente dito. os pr\u00f3ximos 3 caracteres indicam as permiss\u00f5es atribu\u00eddas ao usu\u00e1rio dono do arquivo. rw- permiss\u00e3o para leitura e escrita. os pr\u00f3ximos tr\u00eas caracteres indicam as permiss\u00f5es atribu\u00eddas aos grupos de usu\u00e1rios aos quais o arquivo pertence rw- permiss\u00e3o para leitura e escrita os \u00faltimos 3 caracteres indicam as permiss\u00f5es atribu\u00eddas a quaisquer outros usu\u00e1rios do sistema r-- permiss\u00e3o para leitura As permiss\u00f5es podem ser alteradas atrav\u00e9s do comando chmod (change moderators). Curiosidade : Os diret\u00f3rios possuem o campo type no seguinte formato: drwxrwxr-x Se a permiss\u00e3o de execu\u00e7\u00e3o (x) for removida de todos os n\u00edveis (u,g,o), ningu\u00e9m poder\u00e1 acessar a pasta, apenas o usu\u00e1rio root .","title":"Aula 54"},{"location":"udemy/linux/cap04-linux-fundamentals/#aula-56","text":"Arquivos do linux pertencem a usu\u00e1rios e grupos de usu\u00e1rios. Apenas o usu\u00e1rio root pode alterar os donos de um arquivo. O dono de um arquivo, ou o grupo de usu\u00e1rios ao qual um arquivo pertence, podem ser alterados pelos comandos: chown e chgrp Curiosidade: Se voc\u00ea for o usu\u00e1rio gaspar e dentro do seu diret\u00f3rio (/home/gaspar) haver um arquivo que pertence ao usu\u00e1rio root, voc\u00ea ainda assim poder\u00e1 fazer o que quiser com esse arquivo, pois o seu diret\u00f3rio /home/gaspar pertence a voc\u00ea e te permite ler, escrever e executar qualquer arquivo que estiver dentro.","title":"Aula 56"},{"location":"udemy/linux/cap04-linux-fundamentals/#aula-57","text":"Para dar permiss\u00f5es espec\u00edficas para um usu\u00e1rios, ou para um grupo de usu\u00e1rio, sobre um arquivo, deve-se usar o recurso Access List Control. Toda vez que um arquivo possuir permiss\u00f5es desse tipo, um \"+\" ser\u00e1 o ultimo caractere do campo type desse arquivo. Exemplos: Para listar as permiss\u00f5es ACL de um arquivo: >> getfacl path/to/file Para adicionar permiss\u00f5es a um usu\u00e1rio em especifico: >> setfacl -m u:userName:rwx /path/to/file Para adicionar permiss\u00f5es a um grupo de usu\u00e1rios espec\u00edfico: >> setfacl -m g:groupName:rw path/to/file Para adicionar permiss\u00f5es a um diret\u00f3rio e todos os seus arquivos filhos: >> setfacl -dm \"rwx\" /path/to/file Para remover todas as permiss\u00f5es ACL: >> setfacl -b /path/to/file","title":"Aula 57"},{"location":"udemy/linux/cap05-system-administration/","text":"Section 5 - System administration Lesson 79 As soon as you open a file using vi, you start in its command mode. There are the following commands you can type in this mode: i insert mode r replace mode d delete an entire line x delete character by character in a line u undo the last change made in command mode /keyword search the keyword in the file :q! quit without saving :wq! quit and save changes you can also do this by pressing shift+z+z When you are in the insert mode, you can make changes in the text. If you want to exit the insert mode, just press ESC. Lesson 80 Vi and Vim are pretty much the same text editor. Some differences between them are the following ones: Vim has a GUI and some enhancements Vi is supported in all linux distributions and Vim does not. Lesson 81 Sed command is used in a variety of usecases. Some of them are the following: replace a string in a file with a new string find and delete a line that contains a string remove empty lines remove the first or n lines in a file replace tabs with spaces show defined lines from a file substitute words within vi editor If you happened to want to replace all occurences of the name Seinfeld by the name Peter in a file, you could do this in the Vi editor by accessing the command mode and typying: >> :%s/Seinfeld/Peter/ That is how you could use the sed command inside the editor. Lesson 82 Every time a new user is created, its records are stored in three different files in the Linux system. They are: /etc/passwd /etc/group /etc/shadow You can see informations about all users accessing the file /etc/passwd. For instance: >> cat /etc/passwd | grep spiderman The command above will output something like: spiderman:x:1002:1002::/home/spiderman:/bin/bash Where: spiderman : name of the user x : it is the user password. As it is encrypted, the letter \"x\" is placed to represent it. 1002 : it is the user ID 1002: it is the user group ID. As the default, every user is created in its own group, with the same name and same ID. :: : it is the description of the user. In this case, the field is empty /home/spiderman : it is the path to the user directory /bin/bash : it is the path to the bash terminal. That means the user has access to it. You can change all the default parameters with the complete command showned below: useradd -g superheros -s /bin/bash -c \"this is the user description\" -m -d /home/spiderman spiderman Afer you create a new user, make sure you also create a password for it, using the command below: passwd spiderman Lesson 83 If you are logged as an user, let's say gaspar, and wants to change to another user, like gabriel, run the command below: >> su - gabriel The system will prompt you to insert the password of the user gabriel. Important note: If you are logged as the root user, you can change the login to any user of the system without even knowing its password. Speaking about the root user, it is important to keep in mind that he is the system administrator. A long time ago, if a user wanted to perform administrative tasks, he had to log in as root using the command \"su -\". Later on, the sudo command was created and, since then, users were allowed to execute commands with administrator privileges as long as they had their user ID registered in the /etc/sudoers file. The root credentials were not needed anymore. Therefore, the /etc/sudoers file determines who can use the sudo command and what they can do with it. To edit this file, you have to run the command: >> visudo You can read more about this file here and here . Lesson 84 If you want to monitor users in a linux System, there are some useful commands for that. # It shows all users logged into the system >> who # It shows a history of user accesses >> last Lesson 85 In a Linux system that receives simultaneous accesses from different users, it may happen a situation in which you need to communicate to some of the users or to broadcast a message to all users who are logged in the system. For that, there are the following commands: # shows all users that are logged in the system >> users # Broadcasts a message to all users that are logged in the system # You write the message and press ctrl+d to broadcast it >> wall # Send a message to a user that is logged in the system >> write userName Lessons 86-87 If you work with only one Linux server, you can manage users using the commands you have learned so far. But, if you have hundreds of Linux servers, how could you manage users of every server? It is impractical to access one by one and configure user accounts. The solution for this cenario is called Directory Service. In this approach, a server hosts a database of users that is shared by all other servers of the system. So, a user in one of the servers will authenticate to the Directory Service and get access to the system. In this ecossystem, there are some import terms to learn: Active Directory It is the name of the Directory Service in Windows Systems. IDM - Identity manager It is the name of the Directory Service in Red Hat Linux Systems. WinBIND Is a tool that allows a Linux system to communicate with a Windows system and vice-versa. this way, a Linux user can authenticate to a Active Directory, and a Windows user can authenticate to a Directory Service LDAP - Light Directory Access Protocol It is the protocol used in the process of authentication OpenLDAP It is the name of the Directory Service in Linux Systems. Lesson 89 Concepts learned in the class: Application - It is also called a Service and is basicaly a software that runs on the computer (NTP, NFS, Apache, Nginx, rsyslog etc...) Script - It is a bunch of instructions wrapped up in a file. Process - Each application running on the system has a process associated to it. I understant it as an instance of the application which consumes system resources. Daemon - It is a type of process, the difference here is that the user does not have access to it. A Daemon keeps running in the background until the system is shut down. Thread - depending of its implementation, a process can have multiple execution branches, each branch is called a thread. Jobs - It is also a type of process but it is started from time to time to execute a task. Lesson 90 Crontab is a file in which you can schedule tasks to be executed periodicaly. You can read more about crontabs here . Linux systems provide some pre-defined cronjobs out of the box. hourly It is folder located at: /etc/cron.hourly In it you can add scripts to be executed in a hourly basis The exact hour is defined in the file: /etc/cron.d/0hourly daily It is a folder located at: /etc/cron.daily In it you can add scripts to be executed in a daily basis The exact day is defined in the file: /etc/anacrontab weekly It is a folder located at: /etc/cron.weekly In it you can add scripts to be executed in a weekly basis The exact day of each week is defined in the file: /etc/anacrontab monthly It is a folder located at: /etc/cron.monthly In it you can add scripts to be executed in a monthly basis The exact day of each month is defined in the file: /etc/anacrontab Lesson 93 All the logs produces by the system and applications running on it are stored in the following folders: /var/log/boot /var/log/chronyd /var/log/cron /var/log/maillog /var/log/secure /var/log/messages /var/log/httpd Lesson 95 Every Linux system has a hostname. It is a good practice to choose a hostname that matches the purpose of the server. The hostname is stored in the file /etc/hostname. You can change the hostname by editing this file, or by running the following command: >> hostnamectl set-hostname newHostname After changing the hostname, the system must be rebooted. Lesson 102 If you want to permanently set environment variables for your user runtime, you have to access the file ~/bashrc and add something like this: TEST=\"123\" EXPORT TEST If you want to set environment variables globaly, that is, for all users, you have to access the following files and do the same as above. They are: /etc/bashrc /etc/profile Take care! Overwritint an existing global environment variable can cause a lot of damage to the system!","title":"**Section 5 - System administration**"},{"location":"udemy/linux/cap05-system-administration/#section-5-system-administration","text":"","title":"Section 5 - System administration"},{"location":"udemy/linux/cap05-system-administration/#lesson-79","text":"As soon as you open a file using vi, you start in its command mode. There are the following commands you can type in this mode: i insert mode r replace mode d delete an entire line x delete character by character in a line u undo the last change made in command mode /keyword search the keyword in the file :q! quit without saving :wq! quit and save changes you can also do this by pressing shift+z+z When you are in the insert mode, you can make changes in the text. If you want to exit the insert mode, just press ESC.","title":"Lesson 79"},{"location":"udemy/linux/cap05-system-administration/#lesson-80","text":"Vi and Vim are pretty much the same text editor. Some differences between them are the following ones: Vim has a GUI and some enhancements Vi is supported in all linux distributions and Vim does not.","title":"Lesson 80"},{"location":"udemy/linux/cap05-system-administration/#lesson-81","text":"Sed command is used in a variety of usecases. Some of them are the following: replace a string in a file with a new string find and delete a line that contains a string remove empty lines remove the first or n lines in a file replace tabs with spaces show defined lines from a file substitute words within vi editor If you happened to want to replace all occurences of the name Seinfeld by the name Peter in a file, you could do this in the Vi editor by accessing the command mode and typying: >> :%s/Seinfeld/Peter/ That is how you could use the sed command inside the editor.","title":"Lesson 81"},{"location":"udemy/linux/cap05-system-administration/#lesson-82","text":"Every time a new user is created, its records are stored in three different files in the Linux system. They are: /etc/passwd /etc/group /etc/shadow You can see informations about all users accessing the file /etc/passwd. For instance: >> cat /etc/passwd | grep spiderman The command above will output something like: spiderman:x:1002:1002::/home/spiderman:/bin/bash Where: spiderman : name of the user x : it is the user password. As it is encrypted, the letter \"x\" is placed to represent it. 1002 : it is the user ID 1002: it is the user group ID. As the default, every user is created in its own group, with the same name and same ID. :: : it is the description of the user. In this case, the field is empty /home/spiderman : it is the path to the user directory /bin/bash : it is the path to the bash terminal. That means the user has access to it. You can change all the default parameters with the complete command showned below: useradd -g superheros -s /bin/bash -c \"this is the user description\" -m -d /home/spiderman spiderman Afer you create a new user, make sure you also create a password for it, using the command below: passwd spiderman","title":"Lesson 82"},{"location":"udemy/linux/cap05-system-administration/#lesson-83","text":"If you are logged as an user, let's say gaspar, and wants to change to another user, like gabriel, run the command below: >> su - gabriel The system will prompt you to insert the password of the user gabriel. Important note: If you are logged as the root user, you can change the login to any user of the system without even knowing its password. Speaking about the root user, it is important to keep in mind that he is the system administrator. A long time ago, if a user wanted to perform administrative tasks, he had to log in as root using the command \"su -\". Later on, the sudo command was created and, since then, users were allowed to execute commands with administrator privileges as long as they had their user ID registered in the /etc/sudoers file. The root credentials were not needed anymore. Therefore, the /etc/sudoers file determines who can use the sudo command and what they can do with it. To edit this file, you have to run the command: >> visudo You can read more about this file here and here .","title":"Lesson 83"},{"location":"udemy/linux/cap05-system-administration/#lesson-84","text":"If you want to monitor users in a linux System, there are some useful commands for that. # It shows all users logged into the system >> who # It shows a history of user accesses >> last","title":"Lesson 84"},{"location":"udemy/linux/cap05-system-administration/#lesson-85","text":"In a Linux system that receives simultaneous accesses from different users, it may happen a situation in which you need to communicate to some of the users or to broadcast a message to all users who are logged in the system. For that, there are the following commands: # shows all users that are logged in the system >> users # Broadcasts a message to all users that are logged in the system # You write the message and press ctrl+d to broadcast it >> wall # Send a message to a user that is logged in the system >> write userName","title":"Lesson 85"},{"location":"udemy/linux/cap05-system-administration/#lessons-86-87","text":"If you work with only one Linux server, you can manage users using the commands you have learned so far. But, if you have hundreds of Linux servers, how could you manage users of every server? It is impractical to access one by one and configure user accounts. The solution for this cenario is called Directory Service. In this approach, a server hosts a database of users that is shared by all other servers of the system. So, a user in one of the servers will authenticate to the Directory Service and get access to the system. In this ecossystem, there are some import terms to learn: Active Directory It is the name of the Directory Service in Windows Systems. IDM - Identity manager It is the name of the Directory Service in Red Hat Linux Systems. WinBIND Is a tool that allows a Linux system to communicate with a Windows system and vice-versa. this way, a Linux user can authenticate to a Active Directory, and a Windows user can authenticate to a Directory Service LDAP - Light Directory Access Protocol It is the protocol used in the process of authentication OpenLDAP It is the name of the Directory Service in Linux Systems.","title":"Lessons 86-87"},{"location":"udemy/linux/cap05-system-administration/#lesson-89","text":"Concepts learned in the class: Application - It is also called a Service and is basicaly a software that runs on the computer (NTP, NFS, Apache, Nginx, rsyslog etc...) Script - It is a bunch of instructions wrapped up in a file. Process - Each application running on the system has a process associated to it. I understant it as an instance of the application which consumes system resources. Daemon - It is a type of process, the difference here is that the user does not have access to it. A Daemon keeps running in the background until the system is shut down. Thread - depending of its implementation, a process can have multiple execution branches, each branch is called a thread. Jobs - It is also a type of process but it is started from time to time to execute a task.","title":"Lesson 89"},{"location":"udemy/linux/cap05-system-administration/#lesson-90","text":"Crontab is a file in which you can schedule tasks to be executed periodicaly. You can read more about crontabs here . Linux systems provide some pre-defined cronjobs out of the box. hourly It is folder located at: /etc/cron.hourly In it you can add scripts to be executed in a hourly basis The exact hour is defined in the file: /etc/cron.d/0hourly daily It is a folder located at: /etc/cron.daily In it you can add scripts to be executed in a daily basis The exact day is defined in the file: /etc/anacrontab weekly It is a folder located at: /etc/cron.weekly In it you can add scripts to be executed in a weekly basis The exact day of each week is defined in the file: /etc/anacrontab monthly It is a folder located at: /etc/cron.monthly In it you can add scripts to be executed in a monthly basis The exact day of each month is defined in the file: /etc/anacrontab","title":"Lesson 90"},{"location":"udemy/linux/cap05-system-administration/#lesson-93","text":"All the logs produces by the system and applications running on it are stored in the following folders: /var/log/boot /var/log/chronyd /var/log/cron /var/log/maillog /var/log/secure /var/log/messages /var/log/httpd","title":"Lesson 93"},{"location":"udemy/linux/cap05-system-administration/#lesson-95","text":"Every Linux system has a hostname. It is a good practice to choose a hostname that matches the purpose of the server. The hostname is stored in the file /etc/hostname. You can change the hostname by editing this file, or by running the following command: >> hostnamectl set-hostname newHostname After changing the hostname, the system must be rebooted.","title":"Lesson 95"},{"location":"udemy/linux/cap05-system-administration/#lesson-102","text":"If you want to permanently set environment variables for your user runtime, you have to access the file ~/bashrc and add something like this: TEST=\"123\" EXPORT TEST If you want to set environment variables globaly, that is, for all users, you have to access the following files and do the same as above. They are: /etc/bashrc /etc/profile Take care! Overwritint an existing global environment variable can cause a lot of damage to the system!","title":"Lesson 102"},{"location":"udemy/linux/cap06-shell-scripting/","text":"Section 6 - Shell scripting Lesson 106 In all operating systems, there's a software called Kernel. This software keeps running in the background and forms an interface between the applications and the hardware (RAM, CPU etc..). The applications interact with the kernel through System Calls . Lesson 107 Above the kernel, there is another component, called Shell, which is an interface between the users and the Kernel. Therefore, the Shell is like a CLI that can be used by users to interact with the Kernel. The GUI is a Shell. Why? because all user interactions with it are translated to Shell commands. Lesson 108 There are the following types of Shell: Gnome It is a GUI KDE It is another type of GUI sh It is a type of command line interface (CLI) to run Shell commands It was created along with the Linux system. bash It is another type of CLI for Shell commands. It stands for \"born again shell\" It is like the \"sh\" but with a lot of new features. Lesson 109 The first line of any bash script must begins with: #!/bin/bash Every shell script, after it has been written, must have executable permissions, otherwise, one won't be able to execute it. You can execute the script in the command line typing something like: # If it is a bash script: ./nameOfTheScript.bash # If it is a shell script: ./nameOfTheScript.sh Lesson 110 It is a good practice to insert in a shell script as many logs as possible to inform the user the progress of the task. You can create such logs using the command \"echo\", like showned in the example below: #!/bin/bash echo \"This is the step one\" echo \"Executing...\" echo \"You are: whoami echo # This echo is used to skip one line echo \"Its done!\" In a script, it is also possible to declare variables, like so: #!/bin/bash name=Gabriel echo \"My name is $name\"","title":"**Section 6 - Shell scripting**"},{"location":"udemy/linux/cap06-shell-scripting/#section-6-shell-scripting","text":"","title":"Section 6 - Shell scripting"},{"location":"udemy/linux/cap06-shell-scripting/#lesson-106","text":"In all operating systems, there's a software called Kernel. This software keeps running in the background and forms an interface between the applications and the hardware (RAM, CPU etc..). The applications interact with the kernel through System Calls .","title":"Lesson 106"},{"location":"udemy/linux/cap06-shell-scripting/#lesson-107","text":"Above the kernel, there is another component, called Shell, which is an interface between the users and the Kernel. Therefore, the Shell is like a CLI that can be used by users to interact with the Kernel. The GUI is a Shell. Why? because all user interactions with it are translated to Shell commands.","title":"Lesson 107"},{"location":"udemy/linux/cap06-shell-scripting/#lesson-108","text":"There are the following types of Shell: Gnome It is a GUI KDE It is another type of GUI sh It is a type of command line interface (CLI) to run Shell commands It was created along with the Linux system. bash It is another type of CLI for Shell commands. It stands for \"born again shell\" It is like the \"sh\" but with a lot of new features.","title":"Lesson 108"},{"location":"udemy/linux/cap06-shell-scripting/#lesson-109","text":"The first line of any bash script must begins with: #!/bin/bash Every shell script, after it has been written, must have executable permissions, otherwise, one won't be able to execute it. You can execute the script in the command line typing something like: # If it is a bash script: ./nameOfTheScript.bash # If it is a shell script: ./nameOfTheScript.sh","title":"Lesson 109"},{"location":"udemy/linux/cap06-shell-scripting/#lesson-110","text":"It is a good practice to insert in a shell script as many logs as possible to inform the user the progress of the task. You can create such logs using the command \"echo\", like showned in the example below: #!/bin/bash echo \"This is the step one\" echo \"Executing...\" echo \"You are: whoami echo # This echo is used to skip one line echo \"Its done!\" In a script, it is also possible to declare variables, like so: #!/bin/bash name=Gabriel echo \"My name is $name\"","title":"Lesson 110"}]}